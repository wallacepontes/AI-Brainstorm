# TODO



## AI Chatbot Market Share Worldwide - January 2026

As of January 2026, the global AI chatbot market has shifted from a near-monopoly to a more competitive landscape, with ChatGPT holding 68% share (down from 87.2% in 2025) and Google Gemini rising rapidly to 18.2%. Increased competition is driven by Gemini's integration, alongside contenders like Perplexity. 

### AI Chatbot Market Share (January 2026 - Similarweb Data)

- ChatGPT (OpenAI): 68%
- Google Gemini: 18.2%
- Others (Perplexity, etc.): 13.8% 

### Key Market Trends (Jan 2026)

- Desktop Market: On desktop platforms, ChatGPT maintains a higher dominance of roughly 79.78%, with Perplexity (8.28%) and Gemini (7.2%) following.
- Rapid Growth: Google Geminis share jumped from 5.4% in Jan 2025 to 18.2% in Jan 2026, marking it as the fastest-growing competitor.
- Market Drivers: Growth is fueled by the demand for multimodal capabilities (text, images, audio) and increased integration of AI agents within enterprise workflows.
- Regional Differences: In specific markets like Brazil, ChatGPT retains a stronger lead, with roughly 85.45% market share in Jan 2026. 

The overall market is experiencing a significant shift away from a single dominant player, with competitors catching up through ecosystem integration and specialized features. 

## Introducing GPT-5

Videos
Introducing GPT-5
YouTube 路 OpenAI
Aug 7, 2025
YouTube 路 OpenAI

1:17:31
GPT-5, a major upgrade to Chat GPT, is now available. It's smarter, faster, more reliable, and more useful than previous models.
Introducing GPT-5
YouTube 路 OpenAI
Aug 8, 2025
YouTube 路 OpenAI

1:30
Surprising developers with GPT-5
YouTube 路 OpenAI
Aug 7, 2025
YouTube 路 OpenAI

6:35
GPT-5 is highly technical, exceptional at code generation, and can be trusted to go wild in a pretty large codebase.
OpenAI introduces GPT-5 to the world (AGI )
YouTube 路 Lucas Montano
Aug 8, 2025

## Needle in a Haystack

The "Needle in a Haystack" test evaluates the ability of a large language model (LLM) to accurately retrieve specific information (the "needle") buried within a long, largely irrelevant text (the "haystack"). It measures a model's long-context recall and comprehension capabilities as the volume of input data increases.

### Key evaluation aspects include:

- Information Recall Accuracy: Can the model find a single, specific fact (e.g., a sentence or a random number) placed randomly within a large document?
- Context Window Limits: The test assesses performance across different lengths, often from 4K to 1 million+ tokens, to see if the model can handle extensive contexts.
- "Lost in the Middle" Phenomenon: It identifies whether the model is better at recalling information at the beginning or end of a prompt rather than in the middle.
- Depth and Position Sensitivity: The test measures how well the model performs depending on where the needle is located in the text (e.g., 0% = start, 100% = end). 

### Variations of the Test:

- Multi-Needle Retrieval Task (M-RT): Evaluates the ability to find multiple pieces of information throughout a long text.
- Multi-Needle Reasoning Task (M-RS): Assesses the model's ability to integrate or reason across multiple pieces of information.
- Multimodal Needle in a Haystack (MM-NIAH): A newer version that evaluates the ability to locate specific text or images within a long multimodal document (text + image/video). 

This test is critical for determining whether an LLM can be trusted to handle long-document analysis, such as reading through entire legal contracts or searching through long-term memory. 